{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pRn704_VklFl"
   },
   "source": [
    "# Lecture 5.3: Evaluation\n",
    "\n",
    "This lecture, we are going to evaluate sklearn models.\n",
    "\n",
    "**Learning goals:**\n",
    "- split train, validation, and test set with sklearn\n",
    "- run end to end machine learning experiments\n",
    "- compare model quality\n",
    "- tune a hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oLByC0onklFm"
   },
   "source": [
    "##  1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An estimated [\\$70 million](https://en.wikipedia.org/wiki/Counterfeit_United_States_currency) in counterfeit bills are currently in circulation in the USA. That's quite a hustle üòéüí∞. The Federal Reserve doesn't like it however, and wants our help detecting fake banknotes. This can be a hard task: tiny defects are tough to spot, and counterfeiters constantly change their techniques. \n",
    "\n",
    "Machine Learning models can help, because they performs particularly well on unseen data. The [banknote authentication dataset](https://archive.ics.uci.edu/ml/datasets/banknote+authentication) frames this challenge as a binary classification task.  Let's evaluate and compare ML models trained on this fake/genuine banknote dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSalWjEAklFn"
   },
   "source": [
    "Let's follow the checklist from the lecture slides.\n",
    "\n",
    "## 2. ü§î define ML task\n",
    "\n",
    "As defined above: we are trying to solve a _binary classification task_: fake vs genuine banknotes.\n",
    "\n",
    "## 3. üîç assess model feasibility\n",
    "\n",
    "Detecting fake banknote is a pretty hard problem, but can be done by human experts. ML is also particularly good at detecting low level patterns in images. We also know that this is a solved problem, and have a dataset available. This task is therefore feasible!\n",
    "\n",
    "## 4. üìÇ find data you want to do well on\n",
    "\n",
    "The banknote authentication dataset is a good representation of the bills we might encounter in the \"wild\". We can load it into a `Dataframe`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R8lX5suTklFs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('banknote.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row is a banknote example, and there are four numerical features, `feature_i`, and one binary label, `is_fake`. It might be surprising that these examples aren't images. Instead, they are [wavelet transforms](https://en.wikipedia.org/wiki/Wavelet_transform) of the banknote pictures.\n",
    "\n",
    "All four features have $mean = 0$ and $std = 1$ : they have already been standardized. The `count` row of the summary statistic table shows that there are no missing values. This means no further data preprocessing is necessary, and we can directly train our classifiers. üèÉ‚Äç‚ôÇÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J_jlhuhSklFu"
   },
   "source": [
    "## 6. ‚úÇÔ∏è split a test set and set it aside\n",
    "\n",
    "We usually jump straight into converting this `DataFrame` to features, which we then use to `.fit()` our model. This time however, we first split a test set.\n",
    "\n",
    "sklearn makes this easy with the `train_test_split` function. The [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) mentions that it can split many different inputs:\n",
    "\n",
    "> Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.\n",
    "\n",
    "We choose to split our `DataFrame` 80%/20%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xh-570xUklFv"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.20, random_state=777)\n",
    "print(f'total size: {len(df)}, train set size: {len(train_df)}, test set size: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TwNkHOaHklFx"
   },
   "source": [
    "We choose to \"set aside\" the test set for later use. This prevents us from accidentally using data from the test set during development:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OXmnuvZvklFy"
   },
   "outputs": [],
   "source": [
    "train_df.to_csv('banknote_train.csv', index=False)\n",
    "test_df.to_csv('banknote_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LIA7_-yeklF0"
   },
   "source": [
    "## 7. ‚úÇÔ∏è split train & validation sets\n",
    "\n",
    "We choose to split the validation set _lazily_ , meaning we won't save it to disk like the test set. This is fine, because validation sets _can_ be reused.  \n",
    "i.e Our results won't be statistically compromised, if the split isn't the same for each round of experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jStMAblHklF1"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('banknote_train.csv')\n",
    "train_df, val_df = train_test_split(df, test_size=0.20, random_state=4242)\n",
    "print(f'train set size: {len(train_df)}, validation set size: {len(val_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-U8R4ZYklF3"
   },
   "source": [
    "## 8. üéØ define single number metric\n",
    "\n",
    "We are dealing with a balanced binary classification task, and therefore choose accuracy as our single number metric. This sole number will define our model quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AtFwZ22FklF4"
   },
   "source": [
    "## 9. üîÅ train + validate until happy with losses and metric(s)\n",
    "\n",
    "We are now ready to experiment! Let's first create features and labels. We could use all four features, but it turns out that classification task is then too easy, and it wouldn't be interesting to compare training and validation metrics üòë.\n",
    "\n",
    "So instead, we'll pick features 2 & 4 to spice up the task difficulty üå∂Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYMKTXIWklF4"
   },
   "outputs": [],
   "source": [
    "def to_features(df):\n",
    "    X = df[['feature_2', 'feature_4']].values\n",
    "    y = df['is_fake'].values\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = to_features(train_df)\n",
    "X_val, y_val = to_features(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9nwWjcvsklF8"
   },
   "source": [
    "For our first round of experiments, we'd like to know which type of model best solves our task. We'll use three different classifiers:\n",
    "- logistic regression\n",
    "- random forest\n",
    "- SVM with RBF kernel\n",
    "\n",
    "‚ÑπÔ∏è Don't worry if you haven't heard of the last two models before! There is a whole cornucopia of ML models out there - with new ones published everyday. But a good place to start is with all the [sklearn models](https://scikit-learn.org/stable/supervised_learning.html). Not only is their documentation a great place to learn about how they work and how to use them, but sklearn also has many additional resources, like this [classifier comparison chart](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html). And they mostly follow the sklearn `.fit()` and `.predict()` API, so you can try them out straight away! The best way to learn about models is to explore which are commonly used when you encounter a new ML task. \n",
    "\n",
    "We fit these models on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9SwsEiRkklF8"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=0).fit(X_train, y_train)\n",
    "svm_clf = SVC(kernel='rbf', C=1000, random_state=0).fit(X_train, y_train)\n",
    "lr_clf = LogisticRegression(random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hDyugZEoklF_"
   },
   "source": [
    "We now want to calculate the _accuracy_ of our models. sklearn provides many metric functions in the [`sklearn.metrics`](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) module, including `accuracy_score()`. It compares labels and predictions, so we can use the `.predict()` method of the model api. For example, for our linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "senfWBEIklF_"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# predict labels\n",
    "y_predict = lr_clf.predict(X_val)\n",
    "# compare them to true labels\n",
    "accuracy_score(y_val, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IeO0NgTklGB"
   },
   "source": [
    "69% accuracy, not bad!\n",
    "\n",
    "üß† What does `accuracy` represent? How does one calculate it?\n",
    "\n",
    "Since it is such a common usecase, sklearn makes evaluation even easier by assigning default metrics to popular tasks and model types. For _classifiers_ , the default metric is already accuracy, so we can use the `.score()` method from the model api directly. sklearn will predict labels and compare them to the true labels for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JKlUjZUTklGC"
   },
   "outputs": [],
   "source": [
    "lr_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vDBJx2ymklGE"
   },
   "source": [
    "Now that we know how to evaluate sklearn models, lets's compare all of our banknote classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PW5gkD3gklGF"
   },
   "outputs": [],
   "source": [
    "clfs = [rf_clf, svm_clf, lr_clf]\n",
    "\n",
    "for clf in clfs:\n",
    "    accuracy = clf.score(X_val, y_val)\n",
    "    print(f'classifier: {type(clf).__name__}, validation accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mp3iTqPSklGH"
   },
   "source": [
    "Wow, these models are pretty good! ü§©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HPZ7doJHklGI"
   },
   "source": [
    "Let's carry out a second round of experiments to determine optimal SVM hyperparameter. We're particularly interested in `C` which controls regularization.\n",
    "\n",
    "üí™ Train 6 SVMs, then compare their training & validation accuracy.\n",
    "- use the `C` values listed below\n",
    "- store the training accuracies in a list called `train_accuracies`\n",
    "- store the validation accuracies in a list called `val_accuracies`\n",
    "- use the unit test to debug and verify your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GnDDLUOXklGI"
   },
   "outputs": [],
   "source": [
    "c_values = [0.1, 1, 10, 100, 1000]\n",
    "\n",
    "#¬†INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yhcIyDNdklGK"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def print_results(c_values, train_accuracies, val_accuracies):\n",
    "    for c, train_acc, val_acc in zip(c_values, train_accuracies, val_accuracies):\n",
    "        print(f'C: {c}, train acc: {train_acc}, val acc: {val_acc}')\n",
    "        \n",
    "        \n",
    "def test_svm_C_tuning():\n",
    "    assert train_accuracies, \"Can't find train_accuracies. Did you use the correct variable name?\"\n",
    "    assert val_accuracies, \"Can't find val_accuracies. Did you use the correct variable name?\"\n",
    "    assert len(train_accuracies) == 5, f\"Expected 5 training accuracies, got {len(train_accuracies)}\"\n",
    "    assert len(val_accuracies) == 5, f\"Expected 5 validation accuracies, got {len(val_accuracies)}\"\n",
    "    print_results(c_values, train_accuracies, val_accuracies)\n",
    "    assert math.isclose(4.221208, sum(train_accuracies), rel_tol=1e-5), \"Something is wrong with your training accuracy values\"\n",
    "    assert math.isclose(4.431818, sum(val_accuracies), rel_tol=1e-5), \"Something is wrong with your validation accuracy values\"\n",
    "    print('Success! üéâ')\n",
    "    \n",
    "test_svm_C_tuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ipxQOWwnklGN"
   },
   "source": [
    "üß† What is the best value for the hyperparameter `C`?\n",
    "\n",
    "üß† For which value of `C` does the SVM seem to start overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WKhubImyklGN"
   },
   "source": [
    "## 10. üìè evaluate model on test set to get final metric\n",
    "\n",
    "The SVM is our fake banknote detection model of choice. The International Monetary Fund would like guarantees about how well this model is going to perform in production. To know the expectation value of accuracy on unseen examples, we decide to use our _test set_ to measure the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9hNTncEVklGN"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('banknote_test.csv')\n",
    "X_test, y_test = to_features(test_df)\n",
    "svm_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H9m504S0klGP"
   },
   "source": [
    "üß†üß† The test accuracy is slightly lower than the validation accuracy.\n",
    "- What does test accuracy < validation accuracy usually indicate?\n",
    "- Is the difference significant in this case? \n",
    "- How would you verify this?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "akf45BAuklHD"
   },
   "source": [
    "## 4. Summary\n",
    "\n",
    "Today, we learned about **evaluation methods**. First, we noted that training loss makes for a bad model quality metric, since it cannot detect **overfitting**. We introduced the idea of a held-out **test set** to better estimate generalization properties on unseen examples. We highlighted how test sets work if they are of the **same distribution** as the data we will encounter at prediction time, and if they are **large enough**. We then described how an independent test set can still be prone to overfitting if used as part of **model development**. Since machine learning development is **experimental** & **iterative** in nature, the data scientist introduces an **information leak** between the test set and the model hyperparameters. We introduced the **validation set** as a solution. We split the responsibilities of **comparing** models, and **assessing** models, which allows engineers to both develop and measure the quality machine learning solutions. We then showed that losses weren't always interpretable values, and introducted new **metrics**, like classification accuracy or regression MSE. We underlined the importance of choosing a **single number metric** to define model quality, and speed up model development. We then synthesized all these new workflows into a **ML development checklist**, which captures the steps of typical ML engineering experiments. Finally, we applied this checklist and built a viable ML solution from scratch for banknote authentication in sklearn.\n",
    "\n",
    "\n",
    "# Resources\n",
    "\n",
    "## Core Resources\n",
    "\n",
    "- [Machine learning yearning](https://www.deeplearning.ai/machine-learning-yearning/)  \n",
    "The Andrew Ng reference for ML engineering, including terse and practical sections about validation and test sets\n",
    "- [sklearn on evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html)  \n",
    "Verbose official documentation on sklearn evaluation methods and apis \n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Google ML crash course - accuracy](https://developers.google.com/machine-learning/crash-course/classification/accuracy)  \n",
    "Intuitive explanation of the accuracy metric and its equation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "QlqbGctBklGs"
   ],
   "name": "evaluation_pt.1.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
